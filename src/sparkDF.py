import sys
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, count
from pyspark import StorageLevel

# CONFIGURATION
BUCKET = "gs://pagerbucket10" 
INPUT_DIR = f"{BUCKET}/cleaned_data"
ITERATIONS = 10

if __name__ == "__main__":
    spark = SparkSession.builder.appName("PageRankDataFrame").getOrCreate()

    # On d√©sactive le Broadcast pour √©viter l'explosion RAM sur les petites machines
    spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
    
    # Pour aider le planificateur, on lui dit de pr√©f√©rer le SortMergeJoin s'il h√©site
    spark.conf.set("spark.sql.join.preferSortMergeJoin", "true")

    start_time = time.time()

    # 1. CHARGEMENT
    links = spark.read.parquet(INPUT_DIR)

    # 2. PRE-CALCUL & PARTITIONNEMENT 
    # On calcule les degr√©s sortants
    out_degrees = links.groupBy("src").agg(count("*").alias("out_degree"))
    
    # On joint.
    links_with_degree = links.join(out_degrees, "src")
    
    # On repartitionne par "src", on Shuffle avant la boucle.
    #    Spark sait que les donn√©es sont rang√©es par URL.
    # 20 partitions => valeur arbitraire
    links_partitioned = links_with_degree.repartition(200, "src")
    
    # On persiste sur DISQUE et MEMOIRE.
    # Si √ßa ne tient pas en RAM, √ßa ira sur le disque sans planter.
    links_partitioned.persist(StorageLevel.MEMORY_AND_DISK)
    
    # On force le calcul maintenant pour que le temps de "pr√©paration" ne soit pas compt√© dans la boucle
    # et pour v√©rifier que le cache est bien rempli.
    count_links = links_partitioned.count()
    print(f"Graph charg√© avec {count_links} liens.")

    # 3. INITIALISATION DES RANGS
    ranks = links_partitioned.select("src").distinct().withColumn("rank", lit(1.0))

    # 4. BOUCLE PAGERANK
    print("D√©but du calcul it√©ratif...")
    loop_start = time.time()
    
    for i in range(ITERATIONS):
        # Ici, 'links_partitioned' est d√©j√† sur le bon noeud gr√¢ce au repartition("src").
        # Seul 'ranks' va bouger √† travers le r√©seau.
        contributions = links_partitioned.join(ranks, "src") \
             .select(col("dst").alias("page"), (col("rank") / col("out_degree")).alias("contribution"))
        
        ranks = contributions.groupBy("page").sum("contribution") \
            .withColumn("rank", 0.15 + 0.85 * col("sum(contribution)")) \
            .select(col("page").alias("src"), "rank")

    # 5. RESULTAT
    best = ranks.orderBy(col("rank").desc()).first()
    
    end_time = time.time()

    print(f"‚è±Ô∏è TEMPS BOUCLE (DataFrame): {end_time - loop_start:.2f} secondes")
    print(f"‚è±Ô∏è TEMPS TOTAL (Inc. Chargement): {end_time - start_time:.2f} secondes")
    
    if best:
        print(f"üèÜ GAGNANT: {best['src']} avec un score de {best['rank']}")

    spark.stop()